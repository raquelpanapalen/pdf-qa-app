# SceneScanner - PDF Question Answering for Movie Content

SceneScanner is a full-stack application that lets you upload movie-related PDF documents (like scripts, reviews, etc.), then ask natural language questions about their content. It uses a React frontend and a Flask backend powered by vector search and a local language model via Ollama.

## Setup Instructions

1. **Clone the repository:**
    ```bash
    git clone https://github.com/raquelpanapalen/scenescanner.git
    cd scenescanner
    ```

2. **Install dependencies:**

    ```bash
    # For backend
    cd backend
    python -m venv venv
    source venv/bin/activate
    pip install -r requirements.txt

    # For frontend
    cd frontend
    npm install
    ```

    ### Model Configuration
    This application supports two distinct ways to utilize language models, and you can configure both to compare their performance or suitability for different tasks:

    #### Option 1: Local Open-Source Models (Ollama)
    If you prefer to use open-source models running locally on your machine, you will need to install Ollama and pull the required models.

    1. **Install Ollama:**
    Follow the installation instructions for your operating system on the [Ollama website](https://ollama.com/download).

    2. **Pull Ollama Models:**
    Once Ollama is installed and running, use the following commands in your terminal to download the necessary models:
        ```bash
        ollama pull qwen2.5:latest              # (4.7 GB)
        ollama pull nomic-embed-text:latest     # (274 MB)
        ```
        
        - `qwen2.5` will be used as the Large Language Model (LLM) for generating replies.
        - `nomic-embed-text` will be used to create text embeddings for the vector database.

    #### Option 2: OpenAI Cloud Models
    If you prefer to use OpenAI's powerful cloud-based models, you will need an OpenAI API key.

    1. **Obtain an OpenAI API Key:**
    If you don't have one, you can generate an API key from the OpenAI Platform.

    2. **Configure API Key:**
    Create a file named .env in the root directory of your application (if it doesn't already exist) and add your OpenAI API key to it in the following format:

        `OPENAI_API_KEY="your-api-key-here"`

        Replace the value with your actual OpenAI API key.

        - `gpt-4o-mini` will be used as the LLM for generating replies.
        - `text-embedding-3-large` will be used to create the embeddings for the vector database.



3. **Run the backend server:**
    ```bash
    cd backend
    flask run
    ```

4. **Run the frontend (different terminal):**
    ```bash
    cd frontend
    npm start
    ```

5. **Access the app:**
    - Open [http://localhost:3000](http://localhost:3000) in your browser.



## üìÅ Project Structure
```
project-root/
‚îÇ
‚îú‚îÄ‚îÄ backend/
‚îÇ   ‚îú‚îÄ‚îÄ app.py             # Flask backend
‚îÇ   ‚îú‚îÄ‚îÄ requirements.txt   # Python dependencies
‚îÇ
‚îú‚îÄ‚îÄ frontend/
‚îÇ   ‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ App.jsx        # React UI
‚îÇ   ‚îî‚îÄ‚îÄ package.json       # React dependencies
‚îÇ
‚îî‚îÄ‚îÄ README.md              # This file
```

---

## Code Overview

### i. Overall Architecture
- Frontend (React) lets users upload a PDF and submit questions.
- Backend (Flask) handles PDF processing, vector storage, and LLM-based answering.
- FAISS stores vector representations of document chunks.
- Ollama runs a local LLM (e.g. Qwen2.5) for efficient inference.

### ii. Frontend ‚Üî Backend Interaction
- `/upload`: React sends the selected PDF using a `POST` request with `FormData`.
- Flask reads and splits the PDF into chunks, generates embeddings, and saves them to disk using FAISS.
- A session cookie is used to associate users with their FAISS vector store.
- `/ask`: React sends a natural language question using a `POST` request.
- Flask loads the relevant vector store, retrieves top-k matching chunks, and sends the context to the LLM.

### iii. Backend ‚Üî Ollama and PDF Handling
- PDF Handling: `PyMuPDF` is used via LangChain's `PyMuPDFLoader` to read and chunk the PDF text.
- Embeddings: `OllamaEmbeddings` generates vector representations of text.
- Similarity Search: FAISS performs fast similarity lookups for relevant chunks.
- LLM Prompting: LangChain passes retrieved context and user question to an Ollama-powered model.
- Answering: The LLM returns an answer, which is sent back to the frontend.

### Why LangChain
LangChain offers smooth integration of vector databases, language models, and question-answering chains‚Äîideal for building RAG pipelines.

In our case, we did not have access to OpenAI API credentials, so we opted to use local models instead. While attempting to use Hugging Face embeddings, we encountered several compatibility issues stemming from recent API changes‚ÄîLangChain's current version has deprecation problems and is not fully up to date with the latest Hugging Face methods.

If OpenAI credentials were available, LangChain would have allowed for straightforward integration. By simply providing an API key, the framework can automatically handle connections to OpenAI's services, including both OpenAIEmbeddings and OpenAI LLMs, making setup and deployment significantly smoother.

## üõ°Ô∏è Environment & Security Notes
- The session uses a securely generated `SECRET_KEY`.
- Uploaded PDFs are saved temporarily and removed after processing.
- Cookies are handled securely via Flask sessions (withCredentials in Axios).